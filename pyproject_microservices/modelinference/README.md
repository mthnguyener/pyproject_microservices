#  Model Inference
   - Performs real-time predictions using deployed models.
   - Implements low-latency processing for real-time applications.
   - Note: Can be combined with `Model Serving` for simplicity and reduced latency